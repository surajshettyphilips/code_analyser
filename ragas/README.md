# RAGAs Evaluation Pipeline

This folder contains scripts for batch evaluation of questions using the RAGAs framework.

## Overview

The pipeline performs two main tasks:
1. **Stage 2 Query Processing**: Iterates through questions in an Excel file, queries Stage 2 (retrieval + analysis), and stores AI responses and contexts
2. **RAGAs Evaluation**: Evaluates the quality of AI responses against ground truth using RAGAs metrics

## Files

- `evaluate_questions.py` - Main evaluation script
- `sample_questions.xlsx` - Sample input file with question format
- `README.md` - This file

## Input Excel Format

The input Excel file must have these columns:
- `question` - The question to ask
- `ground_truth` - The correct/expected answer
- `answer` - (Optional) Same as ground_truth for compatibility

Example:
```
question | ground_truth | answer
---------|--------------|--------
Is this code creating spark session? | Yes, the code creates... | Yes, the code creates...
```

## Output Excel Format

The output Excel file will have additional columns:
- `AI_response` - Answer generated by the system (Stage 2)
- `contexts` - Retrieved code chunks that were used
- `ragas_context_precision` - RAGAs metric
- `ragas_context_recall` - RAGAs metric
- `ragas_faithfulness` - RAGAs metric
- `ragas_answer_relevancy` - RAGAs metric

## Usage

### Basic Usage

```powershell
.\venv\Scripts\python.exe ragas/evaluate_questions.py --input ragas/sample_questions.xlsx
```

This will:
1. Read questions from `sample_questions.xlsx`
2. Query Stage 2 for each question
3. Save intermediate results to `sample_questions_evaluated.xlsx`
4. Run RAGAs evaluation
5. Save final results with metrics to `sample_questions_evaluated.xlsx`

### Custom Output Path

```powershell
.\venv\Scripts\python.exe ragas/evaluate_questions.py --input ragas/questions.xlsx --output ragas/results.xlsx
```

### From VS Code

Add this to `.vscode/launch.json`:

```json
{
    "name": "RAGAs: Evaluate Questions",
    "type": "python",
    "request": "launch",
    "module": "ragas.evaluate_questions",
    "console": "integratedTerminal",
    "cwd": "${workspaceFolder}",
    "args": [
        "--input",
        "ragas/sample_questions.xlsx"
    ],
    "justMyCode": false
}
```

## Prerequisites

1. **ChromaDB must be indexed** - Run Stage 1 first to index your code:
   ```powershell
   .\venv\Scripts\python.exe main.py --mode process --file examples/example_pyspark_etl.py
   ```

2. **Ollama must be running** with CodeLlama:7b:
   ```powershell
   ollama list
   ollama pull codellama:7b
   ```

3. **Install openpyxl** for Excel support (if not already installed):
   ```powershell
   .\venv\Scripts\activate
   pip install openpyxl
   ```

## Process Flow

```
1. Load questions from Excel
   ↓
2. For each question:
   - Query ChromaDB for relevant chunks
   - Use DSPy + CodeLlama to generate answer
   - Store AI_response and contexts
   ↓
3. Save intermediate Excel with AI responses
   ↓
4. Run RAGAs evaluation on all Q&A pairs
   ↓
5. Save final Excel with RAGAs metrics
```

## RAGAs Metrics Explained

- **Context Precision**: How relevant are the retrieved chunks?
- **Context Recall**: Does the context contain info to answer the question?
- **Faithfulness**: Is the answer grounded in the retrieved context?
- **Answer Relevancy**: How relevant is the answer to the question?

## Notes

- The script preserves the intermediate results, so you can review AI responses before RAGAs evaluation
- Contexts are stored as text separated by `\n---\n` for readability
- RAGAs metrics are computed over the entire dataset (not per-question)
- For better results with RAGAs, consider using OpenAI models instead of Ollama

## Troubleshooting

**Error: Missing required columns**
- Ensure your Excel has `question` and `ground_truth` columns

**Error: ChromaDB collection empty**
- Run Stage 1 first to index your code

**Low RAGAs scores**
- This is normal with Ollama; RAGAs works better with OpenAI models
- Focus on `answer_relevancy` and `faithfulness` metrics

**Memory issues**
- Process questions in smaller batches
- Reduce `n_results` in `query_stage2()` method
