{
  "collection_name": "pyspark_code_chunks",
  "total_chunks": 1,
  "last_updated": "2025-11-23T12:51:38.540912",
  "indexed_chunks": [
    {
      "id": "example_pyspark_etl.py_chunk_0",
      "document": "\"\"\"\nExample PySpark script for data processing.\nThis demonstrates a typical PySpark ETL pipeline with business logic.\n\"\"\"\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, sum as spark_sum, avg, count\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\nfrom datetime import datetime\n\n\ndef create_spark_session(app_name: str = \"DataProcessingApp\"):\n    \"\"\"\n    Create and configure a Spark session.\n    \n    Business Rule: Application runs in local mode for development.\n    \"\"\"\n    spark = SparkSession.builder \\\n        .appName(app_name) \\\n        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n        .config(\"spark.default.parallelism\", \"4\") \\\n        .master(\"local[*]\") \\\n        .getOrCreate()\n    \n    spark.sparkContext.setLogLevel(\"WARN\")\n    return spark\n\n\ndef load_customer_data(spark, file_path: str):\n    \"\"\"\n    Load customer data from CSV file.\n    \n    Business Rule: Customer data must have specific schema for validation.\n    \"\"\"\n    schema = StructType([\n        StructField(\"customer_id\", StringType(), False),\n        StructField(\"name\", StringType(), False),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"country\", StringType(), True),\n        StructField(\"purchase_amount\", DoubleType(), True),\n        StructField(\"loyalty_status\", StringType(), True)\n    ])\n    \n    df = spark.read.csv(file_path, header=True, schema=schema)\n    return df\n\n\ndef apply_business_rules(df):\n    \"\"\"\n    Apply business rules for customer segmentation.\n    \n    Business Rules:\n    1. Customers with purchase_amount > 1000 are \"Premium\"\n    2. Customers with purchase_amount between 500-1000 are \"Standard\"\n    3. Customers with purchase_amount < 500 are \"Basic\"\n    4. Customers aged > 60 get a 15% senior discount\n    5. Customers aged 18-30 get a 10% youth discount\n    6. Premium customers in loyalty program get additional 5% discount\n    \"\"\"\n    # Rule 1-3: Customer Segmentation\n    df = df.withColumn(\n        \"customer_segment\",\n        when(col(\"purchase_amount\") > 1000, \"Premium\")\n        .when((col(\"purchase_amount\") >= 500) & (col(\"purchase_amount\") <= 1000), \"Standard\")\n        .otherwise(\"Basic\")\n    )\n    \n    # Rule 4-5: Age-based discounts\n    df = df.withColumn(\n        \"discount_rate\",\n        when(col(\"age\") > 60, 0.15)\n        .when((col(\"age\") >= 18) & (col(\"age\") <= 30), 0.10)\n        .otherwise(0.0)\n    )\n    \n    # Rule 6: Additional loyalty discount for premium customers\n    df = df.withColumn(\n        \"discount_rate\",\n        when(\n            (col(\"customer_segment\") == \"Premium\") & (col(\"loyalty_status\") == \"Gold\"),\n            col(\"discount_rate\") + 0.05\n        ).otherwise(col(\"discount_rate\"))\n    )\n    \n    # Calculate final amount after discount\n    df = df.withColumn(\n        \"final_amount\",\n        col(\"purchase_amount\") * (1 - col(\"discount_rate\"))\n    )\n    \n    return df\n\n\ndef filter_valid_customers(df):\n    \"\"\"\n    Filter customers based on validation rules.\n    \n    Business Rules:\n    1. Age must be between 18 and 120\n    2. Purchase amount must be positive\n    3. Country must not be null\n    \"\"\"\n    valid_df = df.filter(\n        (col(\"age\").between(18, 120)) &\n        (col(\"purchase_amount\") > 0) &\n        (col(\"country\").isNotNull())\n    )\n    \n    return valid_df\n\n\ndef calculate_metrics_by_country(df):\n    \"\"\"\n    Calculate key business metrics grouped by country.\n    \n    Business Metrics:\n    - Total revenue (sum of final amounts)\n    - Average customer age\n    - Customer count\n    - Average discount rate\n    \"\"\"\n    metrics = df.groupBy(\"country\").agg(\n        spark_sum(\"final_amount\").alias(\"total_revenue\"),\n        avg(\"age\").alias(\"avg_customer_age\"),\n        count(\"customer_id\").alias(\"customer_count\"),\n        avg(\"discount_rate\").alias(\"avg_discount_rate\")\n    )\n    \n    # Business Rule: Only include countries with at least 10 customers\n    metrics = metrics.filter(col(\"customer_count\") >= 10)\n    \n    return metrics.orderBy(col(\"total_revenue\").desc())\n\n\ndef identify_high_value_customers(df):\n    \"\"\"\n    Identify high-value customers for special campaigns.\n    \n    Business Rule: High-value customers are Premium segment with \n    purchase amount > 2000 OR Gold loyalty status\n    \"\"\"\n    high_value = df.filter(\n        ((col(\"customer_segment\") == \"Premium\") & (col(\"purchase_amount\") > 2000)) |\n        (col(\"loyalty_status\") == \"Gold\")\n    )\n    \n    return high_value.select(\n        \"customer_id\",\n        \"name\",\n        \"customer_segment\",\n        \"purchase_amount\",\n        \"final_amount\",\n        \"loyalty_status\"\n    )\n\n\ndef save_results(df, output_path: str, format: str = \"parquet\"):\n    \"\"\"\n    Save processed data to specified format.\n    \n    Business Rule: Data is partitioned by country for efficient querying.\n    \"\"\"\n    df.write \\\n        .mode(\"overwrite\") \\\n        .partitionBy(\"country\") \\\n        .format(format) \\\n        .save(output_path)\n\n\ndef main():\n    \"\"\"\n    Main ETL pipeline orchestration.\n    \n    Business Process:\n    1. Load customer data\n    2. Apply business rules and segmentation\n    3. Filter valid customers\n    4. Calculate country-level metrics\n    5. Identify high-value customers\n    6. Save results\n    \"\"\"\n    print(f\"Starting ETL pipeline at {datetime.now()}\")\n    \n    # Initialize Spark\n    spark = create_spark_session(\"CustomerAnalyticsPipeline\")\n    \n    try:\n        # Load data\n        print(\"Loading customer data...\")\n        customer_df = load_customer_data(spark, \"data/input/customers.csv\")\n        print(f\"Loaded {customer_df.count()} customer records\")\n        \n        # Apply business rules\n        print(\"Applying business rules...\")\n        processed_df = apply_business_rules(customer_df)\n        \n        # Filter valid customers\n        print(\"Filtering valid customers...\")\n        valid_df = filter_valid_customers(processed_df)\n        print(f\"Valid customers: {valid_df.count()}\")\n        \n        # Calculate metrics\n        print(\"Calculating country metrics...\")\n        country_metrics = calculate_metrics_by_country(valid_df)\n        country_metrics.show()\n        \n        # Identify high-value customers\n        print(\"Identifying high-value customers...\")\n        high_value_customers = identify_high_value_customers(valid_df)\n        print(f\"High-value customers: {high_value_customers.count()}\")\n        high_value_customers.show(10)\n        \n        # Save results\n        print(\"Saving results...\")\n        save_results(valid_df, \"data/output/processed_customers\")\n        save_results(country_metrics, \"data/output/country_metrics\")\n        save_results(high_value_customers, \"data/output/high_value_customers\")\n        \n        print(f\"ETL pipeline completed successfully at {datetime.now()}\")\n        \n    except Exception as e:\n        print(f\"Error in ETL pipeline: {e}\")\n        raise\n    \n    finally:\n        spark.stop()\n\n\nif __name__ == \"__main__\":\n    main()\n",
      "metadata": {
        "chunk_id": "0",
        "start_token": "0",
        "end_token": "1590",
        "total_tokens": "1590",
        "total_file_tokens": "1590",
        "file_path": "C:\\Users\\320196443\\codellama\\examples\\example_pyspark_etl.py",
        "file_name": "example_pyspark_etl.py",
        "file_extension": ".py",
        "source_file": "examples/example_pyspark_etl.py"
      },
      "source_file": "examples/example_pyspark_etl.py",
      "indexed_at": "2025-11-23T12:51:38.540903"
    }
  ]
}